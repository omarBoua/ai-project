{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a01d21df22249b",
   "metadata": {},
   "source": [
    "# First approach, simple neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae4211c5e2026d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:19:56.099950Z",
     "start_time": "2025-01-23T14:19:56.096877Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "import graph\n",
    "from graph import *\n",
    "from node import *\n",
    "from part import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from evaluation import MyPredictionModel, evaluate, load_model, EdgePredictor, OmarPredictionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa4b29a7114e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:17:15.795130Z",
     "start_time": "2025-01-23T14:17:15.037273Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/graphs.dat', 'rb') as file:\n",
    "    train_graphs_list: List[Graph] = pickle.load(file)\n",
    "    train_graphs_list, test_graphs = train_test_split(train_graphs_list, test_size=0.2, random_state=42)\n",
    "all_part_ids = []\n",
    "all_family_ids = []\n",
    "for graph in train_graphs_list:\n",
    "    for n in graph.get_nodes():\n",
    "        all_part_ids.append(int(n.get_part().get_part_id()))\n",
    "        all_family_ids.append(int(n.get_part().get_family_id()))\n",
    "\n",
    "part_vocab_size = max(all_part_ids) + 1\n",
    "family_vocab_size = max(all_family_ids) + 1\n",
    "print(f\"Part Vocab Size: {part_vocab_size}\")\n",
    "print(f\"Family Vocab Size: {family_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:17:17.300889Z",
     "start_time": "2025-01-23T14:17:17.291164Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# 2) Build a PyTorch Dataset\n",
    "############################################################\n",
    "class LinkPredictionDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates positive/negative samples from each Graph.\n",
    "    For each Graph:\n",
    "      - Collect all nodes\n",
    "      - For every pair (i, j), check if it's an edge (label=1) or not (label=0)\n",
    "    \"\"\"\n",
    "    def __init__(self, graphs: List[Graph]):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "\n",
    "        for g in graphs:\n",
    "            # Get the nodes and edges\n",
    "            node_list = g.get_nodes()          # List[Node]\n",
    "            edge_list = g.get_edges()          # List of (Node, Node)\n",
    "            edge_set = self.get_edge_list(edge_list)        # for quick membership checks\n",
    "\n",
    "            # Map node ID -> (part_id, family_id)\n",
    "            node_id_to_features = {}\n",
    "            for node in node_list:\n",
    "                node_id_to_features[node.get_id()] = (\n",
    "                    node.get_part().get_part_id(),\n",
    "                    node.get_part().get_family_id()\n",
    "                )\n",
    "\n",
    "            # We'll gather all node IDs from the node list\n",
    "            node_ids = [n.get_id() for n in node_list]\n",
    "            id_to_node = {n.get_id(): n for n in node_list}\n",
    "\n",
    "            # Create all (i, j) pairs\n",
    "            for i in node_ids:\n",
    "                for j in node_ids:\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    part_i, fam_i = node_id_to_features[i]\n",
    "                    part_j, fam_j = node_id_to_features[j]\n",
    "\n",
    "                    # Sort the pair for an undirected edge check\n",
    "                    pair = tuple(sorted([id_to_node[i], id_to_node[j]],\n",
    "                                        key=lambda x: x.get_id()))\n",
    "                    label = 1 if pair in edge_set else 0\n",
    "\n",
    "                    self.samples.append((int(part_i), int(fam_i), int(part_j), int(fam_j), int(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]  # (part_i, fam_i, part_j, fam_j, label)\n",
    "    def get_edge_list(self, __edges: Dict[Node, List[Node]]):\n",
    "        edge_pairs = set()  # use a set to avoid duplicates\n",
    "\n",
    "        for src, neighbors in __edges.items():\n",
    "            for dst in neighbors:\n",
    "                # Sort the pair so that (NodeA, NodeB) == (NodeB, NodeA)\n",
    "                sorted_pair = tuple(sorted([src, dst], key=lambda n: n.get_id()))\n",
    "                edge_pairs.add(sorted_pair)\n",
    "\n",
    "        return edge_pairs  # Now we have a list of (Node, Node) pairs\n",
    "\n",
    "\n",
    "def train_edge_predictor(model, train_graphs_list, optimizer, criterion, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            part_i, fam_i, part_j, fam_j, label = batch\n",
    "            # Convert to Long / Float for embeddings + BCE\n",
    "            part_i = part_i.long()\n",
    "            fam_i  = fam_i.long()\n",
    "            part_j = part_j.long()\n",
    "            fam_j  = fam_j.long()\n",
    "            label  = label.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(part_i, fam_i, part_j, fam_j)\n",
    "            loss = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587251c30941e698",
   "metadata": {},
   "source": [
    "### Now train it and write it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567de3ed514dde1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:17:38.085942Z",
     "start_time": "2025-01-23T14:17:22.032271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the dataset and dataloader\n",
    "dataset = LinkPredictionDataset(train_graphs_list)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create the model, criterion, and optimizer\n",
    "model_EdgePredictor = EdgePredictor(part_vocab_size, family_vocab_size, embed_dim=16, hidden_dim=32)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model_EdgePredictor.parameters(), lr=0.001)\n",
    "\n",
    "# train the model\n",
    "train_edge_predictor(model_EdgePredictor, train_graphs_list, optimizer, criterion, epochs=50)\n",
    "torch.save(model_EdgePredictor.state_dict(), \"model_EdgePredictor.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec022aa81ee58718",
   "metadata": {},
   "source": [
    "Now evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf78015f64ebea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:18:00.478031Z",
     "start_time": "2025-01-23T14:17:43.635041Z"
    }
   },
   "outputs": [],
   "source": [
    "model_file_path = 'model_EdgePredictor.pth'\n",
    "prediction_model: MyPredictionModel = load_model(model_file_path)\n",
    "\n",
    "# For illustration, we compute the eval score on a portion of the training data\n",
    "instances = [(graph.get_parts(), graph) for graph in test_graphs[:500]]\n",
    "eval_score = evaluate(prediction_model, instances)\n",
    "print(eval_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44575e301020d10a",
   "metadata": {},
   "source": [
    "# Second Method: GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13399306b525fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_predictor(model, train_graphs_list, optimizer, criterion, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"EPOCH:\", epoch)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for graph in train_graphs_list:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Sort nodes\n",
    "            nodes = sorted(\n",
    "                graph.get_nodes(),\n",
    "                key=lambda node: (node.get_part().get_part_id(), node.get_part().get_family_id())\n",
    "            )\n",
    "\n",
    "            # Prepare part/family IDs\n",
    "            part_ids = torch.tensor(\n",
    "                [int(node.get_part().get_part_id()) for node in nodes],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            family_ids = torch.tensor(\n",
    "                [int(node.get_part().get_family_id()) for node in nodes],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "            # Build adjacency on the same device\n",
    "            part_order = tuple(node.get_part() for node in nodes)\n",
    "            adjacency_matrix = torch.tensor(\n",
    "                graph.get_adjacency_matrix(part_order),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(part_ids, family_ids)\n",
    "\n",
    "            # Flatten for loss\n",
    "            target = adjacency_matrix.flatten()\n",
    "            loss = criterion(logits.flatten(), target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_graphs_list)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093450867ae599d",
   "metadata": {},
   "source": [
    "### Now train it and write it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d6aa5ead67d3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:20:39.779321Z",
     "start_time": "2025-01-23T14:20:05.090762Z"
    }
   },
   "outputs": [],
   "source": [
    "model = OmarPredictionModel(part_vocab_size, family_vocab_size, embed_dim=1, gnn_hidden_dim=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_graph_predictor(model, train_graphs_list, optimizer, criterion, epochs=100)\n",
    "torch.save(model.state_dict(), \"graph_predictor_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e1c6ac121dd4d",
   "metadata": {},
   "source": [
    "### Now evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99333a55d05cfd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T14:23:14.449364Z",
     "start_time": "2025-01-23T14:22:58.701846Z"
    }
   },
   "outputs": [],
   "source": [
    "model_file_path = 'graph_predictor_model.pth'\n",
    "prediction_model: MyPredictionModel = load_model(model_file_path)\n",
    "\n",
    "# For illustration, we compute the eval score on a portion of the training data\n",
    "instances = [(graph.get_parts(), graph) for graph in test_graphs[:500]]\n",
    "eval_score = evaluate(prediction_model, instances)\n",
    "print(eval_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
